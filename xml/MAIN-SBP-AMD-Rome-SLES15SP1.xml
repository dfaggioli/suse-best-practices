<?xml version="1.0" encoding="UTF-8"?>
<!--<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>-->
<!DOCTYPE article [
<!ENTITY % entity SYSTEM "entity-decl.ent">
%entity;
]>

<article role="sbp" xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="art.sbp.amdepyc.sles12sp3"
  xml:lang="en">

  <info>
    <title>Optimizing Linux for AMD Rome with SUSE Linux Enterprise 15 SP1</title>
    <!--<subtitle>Simplified Deployment on Microsoft Azure</subtitle>-->
    <productname>SUSE Linux Enterprise Server</productname>
    <productnumber>15 SP1</productnumber>

    <author>
      <personname>
        <firstname>Mel</firstname>
        <surname>Gorman, Senior Kernel Engineer, SUSE</surname>
      </personname>
      <!--      <affiliation>
        <jobtitle>Senior Software Engineer</jobtitle>
        <orgname>SUSE</orgname>
        </affiliation>-->
    </author>

    <author>
      <personname>
        <firstname>Matt</firstname>
        <surname>Fleming, Senior Performance Engineer, SUSE</surname>
      </personname>
      <!--      <affiliation>
        <jobtitle>Senior Software Engineer</jobtitle>
        <orgname>SUSE</orgname>
        </affiliation>-->
    </author>

    <author>
      <personname>
        <firstname>Dario</firstname>
        <surname>Faggioli, Software Engineer Virtualization Specialist, SUSE</surname>
      </personname>
      <!--      <affiliation>
        <jobtitle>Senior Software Engineer</jobtitle>
        <orgname>SUSE</orgname>
        </affiliation>-->
    </author>

    <author>
      <personname>
        <firstname>Martin</firstname>
        <surname>Jambor, Tool Chain Developer, SUSE</surname>
      </personname>
      <!--      <affiliation>
        <jobtitle>Senior Software Engineer</jobtitle>
        <orgname>SUSE</orgname>
        </affiliation>-->
    </author>

    <date><?dbtimestamp format="B d, Y" ?></date>

    <abstract>

      <para>The document at hand provides an overview of the AMD EPYC 2 (TM) architecture and how
        some computational-intensive workloads can be tuned on SUSE Linux Enterprise Server 15
        SP1.</para>
    </abstract>
  </info>

  <sect1 xml:id="sec.overview">
    <title>Overview</title>
    <para>EPYC 2 is the latest generation of the AMD64 System-on-Chip (SoC) processor family. It is
      based on the Zen 2 microarchitecture introduced in 2019, supporting up to 64 cores (128 threads)
      and 8 memory channels per socket. At the time of writing, 1-socket and 2-socket models are
      available from Original Equipment Manufacturers (OEMs). This document provides an overview of
      the EPYC 2 architecture and how computational-intensive workloads can be tuned on SUSE Linux
      Enterprise Server 15 SP1.</para>
  </sect1>

  <sect1 xml:id="sec.epyc_architecture">
    <title>EPYC 2 Architecture</title>

    <para><emphasis role="italic">Symmetric multiprocessing (SMP)</emphasis> systems are those that
      contain two or more physical processing cores. Each core may have two threads if
      Symmetetric Multi-Threading (SMT) is enabled with some resources being shared between SMT siblings.
      To minimize access latencies, multiple layers of caches are used with each level being larger but
      with higher access costs. Cores may share different levels of cache which should be considered
      when tuning for a workload.</para>

    <para>Historically, a single socket contained several cores sharing a hierarchy of caches and
      memory channels and multiple sockets were connected via a memory interconnect. Modern
      configurations may have multiple dies as a <emphasis role="italic">Multi-Chip Module
        (MCM)</emphasis> with one set of interconnects within the socket and a separate interconnect
      for each socket. In practical terms, it means that some CPUs and memory are faster to access
      than others depending on the <quote>distance</quote>. This should be considered when tuning
      for <emphasis role="italic">Non-Uniform Memory Architecture (NUMA)</emphasis> as all memory
      accesses are not necessarily to local memory incurring a variable access penalty.</para>

    <para>EPYC 2 is an MCM design with upto eight dies on each package. The number of cores on
      each die is always symmetric so they are balanced. Each socket has eight memory channels
      (two channels per die) with two <emphasis role="italic">Dual Inline Memory Modules
      (DIMMs)</emphasis> allowed per channel for up to 16 DIMMs per socket. Total
      capacity is expected to be 4TB per socket with a maximum bandwidth of 25.6GB/sec per
      channel for a total of 204.8GB/sec per socket depending on the DIMMs selected.</para>

    <para>Within the package, the eight dies are interconnected with a fully-connected Infinity
      Fabric. Fully connected means that one core accessing memory connected to another die will
      always be one hop away. The bandwidth of the fabric is 64 GB/sec per link. The link is
      optimized for low-power and low-latency so the bandwidth available means that a die accessing
      memory local to the socket incurs a smaller access penalty than is normally expected when
      accessing remote memory.</para>

    <para>Sockets are also connected via Infinity Fabric with eight links between each socket
      connecting each die on one socket to the peer die on the second socket. Consequently, access
      distance to remote memory from a thread will be at most 2 hops away. The data bandwidth on
      each of these links is 64 GB/sec for a total of 512 GB/sec between sockets. At the time of
      writing, only two sockets are possible within a single machine.</para>

    <para>Power management on the links is careful to minimize the amount of power required. If the
      links are idle then the power may be used to boost the frequency of individual cores. Hence,
      it is worth noting that minimising access is not only important from a memory access latency
      point of view, but it also has an impact on the speed of individual cores.</para>

    <para>There are eight IO x16 links per die giving a total of 16 links where links can be used as
      Infinity links, PCI Express links or a limited number of SATA links. This allows very large IO
      configurations and a high degree of flexibility because of having a total of 128 lanes
      available on single socket machines. It is important to note that the number of links
      available is equal in one socket and two socket configurations. In one socket configurations,
      all lanes are available for IO. In two socket configurations, some lanes are used to connect
      the two sockets together with the upshot that a one socket configuration does not compromise
      on the available IO channels.</para>

  </sect1>

  <sect1 xml:id="sec.epyc_topology">
    <title>EPYC 2 Topology</title>

    <para>Figure 1 below shows the topology of an example machine generated by the
        <package>lstopo</package> tool. </para>

    <figure xml:id="fig.epyc_topology">
      <title>EPYC 2 Topology</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="amd-epyc-2-topology.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="amd-epyc-2-topology.png" width="80%" format="PNG"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>This tool is part of the <package>hwloc</package> package which is not supported in SUSE
      Linux Enterprise Server 15 SP1 but can be installed for illustration. The two
        <quote>packages</quote> correspond to each socket. The eight dies on each socket are clearly
      visible and each die has a split L3 cache. Optimizing for computation should focus on
      co-operating tasks being bound to a die. In this example, the IO channels are not heavily used
      but the focus will be CPU and memory-intensive loads. If optimizing for IO, it is recommended,
      where possible, that the workload is located on the nodes local to the IO channel.</para>

    <para>The computer output below shows a conventional view of the topology using the
        <package>numactl</package> tool. The CPU IDs that map to each node are reported on the
        <quote>node X cpus:</quote> lines and note the NUMA distances on the table at the bottom of
      the computer output. Node 0 and node 1 are a distance of 32 apart as they are on
      separate sockets. The distance is a not a guarantee of the access latency but it is a rule of
      thumb that accesses between sockets are roughly twice the cost of accessing another die on the
      same socket.</para>

    <screen>epyc:~ # numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
node 0 size: 128255 MB
node 0 free: 125477 MB
node 1 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255
node 1 size: 111609 MB
node 1 free: 111048 MB
node distances:
node   0   1 
  0:  10  32 
  1:  32  10 
</screen>

    <para>Finally, the cache topology can be discovered in a variety of fashions. While
        <package>lstopo</package> can provide the information, it is not always available.
      Fortunately, the level, size and ID of CPUs that share cache can be identified from the files
      under <filename>/sys/devices/system/cpu/cpuN/cache</filename>.</para>
  </sect1>

  <sect1 xml:id="sec.memory_cpu_binding">
    <title>Memory and CPU Binding</title>

    <para>NUMA is a scalable memory architecture for multiprocessor systems that can reduce
      contention on a memory channel. A full discussion on tuning for NUMA is beyond the scope for
      this paper. But the document <quote>A NUMA API for Linux</quote> at <link
        xlink:href="http://developer.amd.com/wordpress/media/2012/10/LibNUMA-WP-fv1.pdf"
        >http://developer.amd.com/wordpress/media/2012/10/LibNUMA-WP-fv1.pdf</link> provides a
      valuable introduction.</para>

    <para>The default policy for programs is the <quote>local policy</quote>. A program which calls
        <command>malloc()</command> or <command>mmap()</command> reserves virtual address space but
      does not immediately allocate physical memory. The physical memory is allocated the first time
      the address is accessed by any thread and, if possible, the memory will be local to the
      accessing CPU. If the mapping is of a file, the first access may have occurred at any time in
      the past so there are no guarantees about locality.</para>

    <para>When memory is allocated to a node, it is less likely to move if a thread changes to a CPU
      on another node or if multiple programs are remote accessing the data unless <emphasis
        role="italic">Automatic NUMA Balancing (NUMAB)</emphasis> is enabled. When NUMAB is enabled,
      unbound process accesses are sampled and if there are enough remote accesses then the data
      will be migrated to local memory. This mechanism is not perfect and incurs overhead of its
      own. This means it can be important for performance for thread and process migrations between
      nodes to be minimized and for memory placement to be carefully considered and tuned.</para>

    <para>The <package>taskset</package> tool is used to set or get the CPU affinity for new or
      existing processes. An example use is to confine a new process to CPUs local to one node.
      Where possible, local memory will be used. But if the total required memory is larger than the
      node then remote memory can still be used. In such configurations, it is recommended to size
      the workload such that it fits in the node to avoid any of the data being paged out when
        <package>kswapd</package> wakes to reclaim memory from the local node.</para>

    <para><package>numactl</package> controls both memory and CPU policies for processes that it
      launches and can modify existing processes. In many respects, the parameters are easier to
      specify than <package>taskset</package>. For example, it can bind a task to all CPUs on a
      specified node instead of having to specify individual CPUs with <package>taskset</package>.
      Most importantly, it can set the memory allocation policy without requiring application
      awareness.</para>

    <para>Using policies, a preferred node can be specified where the task will use that node if
      memory is available. This is typically used in combination with binding the task to CPUs on
      that node. If a workload's memory requirements are larger than a single node and predictable
      performance is required then the <quote>interleave</quote> policy will round-robin allocations
      from allowed nodes. This gives sub-optimal but predictable access latencies to main memory.
      More importantly, interleaving reduces the probability that the OS will need to reclaim any
      data belonging to a large task.</para>

    <para>Further improvements can be made to access latencies by binding a workload to a single
        <emphasis role="italic">CPU Complex (CCX)</emphasis> within a node. Since L3 caches are not
      shared between CCXs, binding a workload to a CCX avoids L3 cache misses caused by workload
      migration.</para>

    <para>Find examples below on how <package>taskset</package> and <package>numactl</package> can
      be used to start commands bound to different CPUs depending on the topology.</para>

    <screen># Run a command bound to CPU 1
epyc:~ # taskset -c 1 [command]
  
# Run a command bound to CPUs belonging to node 0
epyc:~ # taskset -c `cat /sys/devices/system/node/node0/cpulist` [command]
  
# Run a command bound to CPUs belonging to nodes 0 and 1
epyc:~ # numactl –cpunodebind=0,1 [command]
  
# Run a command bound to CPUs that share L3 cache with cpu 1
epyc:~ # taskset -c `cat /sys/devices/system/cpu/cpu1/cache/index3/shared_cpu_list` [command]</screen>

    <sect2 xml:id="sec.tuning_without_binding">
      <title>Tuning for Local Access Without Binding</title>

      <para>The ability to use local memory where possible and remote memory if necessary is
        valuable but there are cases where it is imperative that local memory always be used. If
        this is the case, the first priority is to bind the task to that node. If that is not
        possible then the command <command>sysctl vm.zone_reclaim_mode=1</command> can be used to
        aggressively reclaim memory if local memory is not available. </para>

      <note>
        <title>High Costs</title>
        <para>While this option is good from a locality perspective, it can incur high costs because
          of stalls related to reclaim and the possibility that data from the task will be
          reclaimed. Treat this option with a high degree of caution and testing.</para>
      </note>
    </sect2>

    <sect2 xml:id="sec.hazards_cpu_binding">
      <title>Hazards with CPU Binding</title>

      <para>There are three major hazards to consider with CPU binding.</para>

      <para>The first is to watch for remote memory nodes being used where the process is not
        allowed to run on CPUs local to that node. While going more in detail here is outside the
        scope of this paper, the most common scenario is an IO-bound thread communicating with a
        kernel IO thread on a remote node bound to the IO controller whose accesses are never local.
        Similarly, the version of <package>irqbalance</package> shipped with SUSE Linux Enterprise
        Server 15 SP1 is not necessarily optimal for EPYC 2. Thus it is worth considering disabling
          <package>irqbalance</package> and manually binding IRQs from storage or network devices to
        CPUs that are local to the IO channel. Depending on the kernel version and drivers in use,
        it may not be possible to manually bind IRQs. For example, some devices multi-queue support
        may not permit IRQs affinities to be changed. </para>

      <para>The second is that guides about CPU binding tend to focus on binding to a single CPU.
        This is not always optimal when the task communicates with other threads as fixed bindings
        potentially miss an opportunity for the processes to use idle cores sharing an L1 or L2
        cache. This is particularly true when dispatching IO, be it to disk or a network interface
        where a task may benefit from being able to migrate close to the related threads but also
        applies to pipeline-based communicating threads for a computational workload. Hence, focus
        initially on binding to CPUs sharing L3 cache and then, and only then, consider whether to
        bind based on a L1/L2 cache or a single CPU using the primary metric of the workload to
        establish whether the tuning is appropriate. </para>

      <para>The final hazard is similar in that if many tasks are bound to a smaller set of CPUs
        then the subset of CPUs could be over-saturated even though the machine overall has spare
        capacity.</para>
    </sect2>

    <sect2 xml:id="sec.cpusets_memory_control_groups">
      <title>CPUsets and Memory Control Groups</title>

      <para><emphasis role="italic">CPUsets</emphasis> are ideal when multiple workloads must be
        isolated on a machine in a predictable fashion. CPUsets allow a machine to be partitioned
        into subsets. These sets may overlap, and in that case they suffer from similar problems as
        CPU affinities. In the event there is no overlap, they can be switched to
          <quote>exclusive</quote> mode which treats them completely in isolation with relatively
        little overhead with the caveat that one overloaded CPUset can be saturated leaving another
        CPUset completely idle. Similarly, they are well suited when a primary workload must be
        protected from interference because of low-priority tasks in which case the low priority
        tasks can be placed in a CPUset. The caveat with CPUsets is that the overhead is higher than
        using scheduler and memory policies. Ordinarily, the accounting code for CPUsets is
        completely disabled but when a single CPUset is created there are additional essential
        checks that are made when checking scheduler and memory policies.</para>

      <para>Similarly <package>memcg</package> can be used to limit the amount of memory that can be
        used by a set of processes. When the limits are exceeded then the memory will be reclaimed
        by tasks within <package>memcg</package> directly without interfering with any other tasks.
        This is ideal for ensuring there is no inference between two or more sets of tasks. Similar
        to CPUsets, there is some management overhead incurred so if tasks can simply be isolated on
        a NUMA boundary then it is preferred from a performance perspective. The major hazard is
        that if the limits are exceeded then the processes directly stall to reclaim the memory
        which can incur significant latencies. </para>

      <note>
        <title/>
        <para>Without <package>memcg</package>, when memory gets low, the global reclaim daemon does
          work in the background and if it reclaims quickly enough, no stalls are incurred. When
          using <package>memcg</package>, observe the <package>allocstall</package> counter in
            <filename>/proc/vmstat</filename> as this can detect early if stalling is a
          problem.</para>
      </note>

    </sect2>
  </sect1>

  <sect1 xml:id="sec.hp_storage_interrupt_affinity">
    <title>High-performance Storage Devices and Interrupt Affinity</title>

    <para>High-performance storage devices like <emphasis role="italic">Non-Volatile Memory Express
        (NVMe)</emphasis> or <emphasis role="italic">Serial Attached SCSI (SAS)</emphasis>
      controller are designed to take advantage of parallel I/O submission. These devices typically
      support a large number of submit and receive queues, which are tied to <emphasis role="italic"
        >MSI-X</emphasis> interrupts. Ideally these devices should provide as many MSI-X vectors as
      CPUs are present in the system. To achieve the best performance each MSI-X vector should be
      assigned to an individual CPU.</para>

    <sect2 xml:id="sec.auto_numa_balancing">
      <title>Automatic NUMA Balancing</title>

      <para>Automatic NUMA Balancing will ignore any task that uses memory policies. If the
        workloads can be manually optimized with policies then do so and disable automatic NUMA
        balancing by specifying <command>numa_balancing=disable</command> on the kernel command line
        or via <command>sysctl</command>. There are many cases where it is impractical or impossible
        to specify policies in which case the balancing should be sufficient for
        throughput-sensitive workloads. For latency sensitive workloads, the sampling for NUMA
        balancing may be too high in which case it may be necessary to disable balancing. The final
        corner case where NUMA balancing is a hazard is a case where the number of runnable tasks
        always exceeds the number of CPUs in a single node. In this case, the load balancer (and
        potentially affine wakes) will constantly pull tasks away from the preferred node as
        identified by automatic NUMA balancing resulting in excessive sampling and CPU
        migrations.</para>
    </sect2>
  </sect1>

  <sect1 xml:id="sec.evaluating_workloads">
    <title>Evaluating Workloads</title>

    <para>The first and foremost step when evaluating how a workload should be tuned is to establish
      a primary metric such as latency, throughput or elapsed time. When each tuning step is
      considered or applied, it is critical that the primary metric be examined before conducting
      any further analysis to avoid intensive focus on the wrong bottleneck. Make sure that the
      metric is measured multiple times to ensure that the result is reproducible and reliable
      within reasonable boundaries. When that is established, analyse how the workload is using
      different system resources to determine what area should be the focus. The focus in this paper
      is on how CPU and memory is used. But other evaluations may need to consider the IO subsystem,
      network subsystem, system call interfaces, external libraries etc. The methodologies that can
      be employed to conduct this are outside the scope of the paper but the book <quote>Systems
        Performance: Enterprise and the Cloud</quote> by Brendan Gregg (see <link
        xlink:href="http://www.brendangregg.com/sysperfbook.html"
        >http://www.brendangregg.com/sysperfbook.html</link>) is a recommended primer on the
      subject.</para>

    <sect2 xml:id="sec.cpu_utilization_saturation">
      <title>CPU Utilization and Saturation</title>

      <para>Decisions on whether to bind a workload to a subset of CPUs require that the CPU
        utilization and any saturation risk is known. Both the <command>ps</command> and
          <command>pidstat</command> commands can be used to sample the number of threads in a
        system. Typically <command>pidstat</command> yields more useful information with the
        important exception of the run state. A system may have many threads but if they are idle
        then they are not contributing to utilization. The <command>mpstat</command> command can
        report the utilization of each CPU in the system. </para>

      <para>High utilization of a small subset of CPUs may be indicative of a single-threaded
        workload that is pushing the CPU to the limits and may indicate a bottleneck. Conversely,
        low utilization may indicate a task that is not CPU-bound, is idling frequently or is
        migrating excessively. While each workload is different, load utilization of CPUs may show a
        workload that can run on a subset of CPUs to reduce latencies because of either migrations
        or remote accesses. When utilization is high, it is important to determine if the system
        could be saturated. The <package>vmstat</package> tool reports the number of runnable tasks
        waiting for CPU in the <quote>r</quote> column where any value over 1 indicates that wakeup
        latencies may be incurred. While the exact wakeup latency can be calculated using trace
        points, knowing that there are tasks queued is an important step. If a system is saturated,
        it may be possible to tune the workload to use fewer threads.</para>

      <para>Overall, the initial intent should be to use CPUs from as few NUMA nodes as possible to
        reduce access latency but there are exceptions. EPYC 2 has an exceptional number of high-speed
        memory channels to main memory, thus consider the workload thread activity. If they are
        co-operating threads or sharing data then isolate them on as few nodes as possible to
        minimize cross-node memory accesses. If the threads are completely independent with no
        shared data, it may be best to isolate them on a subset of CPUs from each node to maximize
        the number of available memory channels and throughput to main memory. For some
        computational workloads, it may be possible to use hybrid models such as MPI for
        parallelization across nodes and using openMP for threads within nodes.</para>
    </sect2>

    <sect2 xml:id="sec.transparent_huge_pages">
      <title>Transparent Huge Pages</title>

      <para>Huge pages are a mechanism by which performance can be improved by reducing the number
        of page faults, the cost of translating virtual addresses to physical addresses because of
        fewer layers in the page table and being able to cache translations for a larger portion of
        memory. <emphasis role="italic">Transparent Huge Pages (THP)</emphasis> is supported for
        private anonymous memory that automatically backs mappings with huge pages where anonymous
        memory could be allocated as <command>heap</command>, <command>malloc()</command>,
          <command>mmap(MAP_ANONYMOUS)</command>, etc. While the feature has existed for a long
        time, it has evolved significantly.</para>

      <para>Many tuning guides recommend disabling THP because of problems with early
        implementations. Specifically, when the machine was running for long enough, the use of THP
        could incur severe latencies and could aggressively reclaim memory in certain circumstances.
        These problems have been resolved by the time SUSE Linux Enterprise Server 15 SP1 was
        released. This means there are no good grounds for automatically disabling THP because of
        severe latency issues without measuring the impact. However, there are exceptions that are
        worth considering for specific workloads.</para>

      <para>Some high-end in-memory databases and other applications aggressively use
          <command>mprotect()</command> to ensure that unprivileged data is never leaked. If these
        protections are at the base page granularity then there may be many THP splits and rebuilds
        that incur overhead. It can be identified if this is a potential problem by using
          <command>strace</command> to detect the frequency and granularity of the system call. If
        they are high frequency then consider disabling THP. It can also be sometimes inferred from
        observing the <command>thp_split</command> and <command>thp_collapse_alloc
          counters</command> in <filename>/proc/vmstat</filename>. </para>

      <para>Workloads that sparsely address large mappings may have a higher memory footprint when
        using THP. This could result in premature reclaim or fallback to remote nodes. An example
        would be HPC workloads operating on large sparse matrices. If memory usage is much higher
        than expected then compare memory usage with and without THP to decide if the tradeoff is
        not worthwhile. This may be critical on EPYC 2 given that any spillover will congest the
        Infinity links and potentially cause cores to run at a lower frequency. </para>

      <note>
        <title>Sparsely Addressed Memory</title>
        <para>This is specific to sparsely addressed memory. A secondary hint for this case may be
          that the application primarily uses large mappings with a much higher Virtual Size (VSZ,
          see <xref linkend="sec.cpu_utilization_saturation"/>) than Resident Set Size (RSS).
          Applications which densely address memory benefit from the use of THP by achieving greater
          bandwidth to memory.</para>
      </note>

      <para>Parallelized workloads that operate on shared buffers with threads using more CPUs that
        are on a single node may experience a slowdown with THP if the granularity of partitioning
        is not aligned to the huge page. The problem is that if a large shared buffer is partitioned
        on a 4K boundary then false sharing may occur whereby one thread accesses a huge page
        locally and other threads access it remotely. If this situation is encountered, it is
        preferable that the granularity of sharing is increased to the THP size. But if that is not
        possible then disabling THP is an option.</para>

      <para>Applications that are extremely latency sensitive or must always perform in a
        deterministic fashion can be hindered by THP. While there are fewer faults, the time for
        each fault is higher as memory must be allocated and cleared before being visible. The
        increase in fault times may be in the microsecond granularity. Ensure this is a relevant
        problem as it typically only applies to hard real-time applications. The secondary problem
        is that a kernel daemon periodically scans a process looking for contiguous regions that can
        be backed by huge pages. When creating a huge page, there is a window during which that
        memory cannot be accessed by the application and new mappings cannot be created until the
        operation is complete. This can be identified as a problem with thread-intensive
        applications that frequently allocate memory. In this case consider effectively disabling
          <package>khugepaged</package> by setting a large value in
          <filename>/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs</filename>.
        This will still allow THP to be used opportunistically while avoiding stalls when calling
          <command>malloc()</command> or <command>mmap()</command>.</para>

      <para>THP can be disabled. To do so, specify <command>transparent_hugepage=disable</command>
        on the kernel command line, at runtime via
          <filename>/sys/kernel/mm/transparent_hugepage/enabled</filename> or on a per process basis
        by using a wrapper to execute the workload that calls
          <command>prctl(PR_SET_THP_DISABLE)</command>.</para>

    </sect2>

    <sect2 xml:id="sec.user_kernel_footprint">
      <title>User/Kernel Footprint</title>

      <para>Assuming an application is mostly CPU or memory bound, it is useful to determine if the
        footprint is primarily in user space or kernel space as it gives a hint where tuning should
        be focused. The percentage of CPU time can be measured on a coarse-grained fashion using
          <command>vmstat</command> or a fine-grained fashion using <command>mpstat</command>. If an
        application is mostly spending time in user space then the focus should be on tuning the
        application itself. If the application is spending time in the kernel then it should be
        determined which subsystem dominates. The <command>strace</command> or <command>perf
          trace</command> commands can measure the type, frequency and duration of system calls as
        they are the primary reasons an application spends time within the kernel. In some cases, an
        application may be tuned or modified to reduce the frequency and duration of system calls.
        In other cases, a profile is required to identify which portions of the kernel are most
        relevant as a target for tuning.</para>
    </sect2>

    <sect2 xml:id="sec.memory_utilization_saturation">
      <title>Memory Utilization and Saturation</title>

      <para>The traditional means of measuring memory utilization of a workload is to examine the
          <emphasis role="italic">Virtual Size (VSZ)</emphasis> and <emphasis role="italic">Resident
          Set Size (RSS)</emphasis> using either the <command>ps</command> or
          <command>pidstat</command> tool. This is a reasonable first step but is potentially
        misleading when shared memory is used and multiple processes are examined. VSZ is simply a
        measure of memory space reservation and is not necessarily used. RSS may be double accounted
        if it is a shared segment between multiple processes. The file
          <filename>/proc/pid/maps</filename> can be used to identify all segments used and whether
        they are private or shared. The file <filename>/proc/pid/smaps</filename> yields more
        detailed information including the <emphasis role="italic">Proportional Set Size
          (PSS)</emphasis>. PSS is an estimate of RSS except it is divided between the number of
        processes mapping that segment which can give a more accurate estimate of utilization. Note
        that the <filename>smaps</filename> file is very expensive to read and should not be
        monitored at a high frequency. Finally, the <emphasis role="italic">Working Set Size
          (WSS)</emphasis> is the amount of memory active required to complete computations during
        an arbitrary phase of a programs execution. It is not a value that can be trivially
        measured. But conceptually it is useful as the interaction between WSS relative to available
        memory affects memory residency and page fault rates.</para>

      <para>On NUMA systems, the first saturation point is a node overflow when the
          <quote>local</quote> policy is in effect. Given no binding of memory, when a node is
        filled, a remote node’s memory will be used transparently and background reclaim will take
        place on the local node. Two consequences of this are that remote access penalties will be
        used and old memory from the local node will be reclaimed. If the WSS of the application
        exceeds the size of a local node then paging and refaults may be incurred.</para>

      <para>The first thing to identify is that a remote node overflow occurred which is accounted
        for in <filename>/proc/vmstat</filename> as the <command>numa_hit</command>,
          <command>numa_miss</command>, <command>numa_foreign</command>,
          <command>numa_interleave</command>, <command>numa_local</command> and <command>numa_other
          counters</command>:</para>

      <itemizedlist>
        <listitem>
          <para><command>numa_hit</command> is incremented when an allocation uses the preferred
            node where preferred may be either a local node or one specified by a memory
            policy.</para>
        </listitem>
        <listitem>
          <para><command>numa_miss</command> is incremented when an alternative node is used to
            satisfy an allocation.</para>
        </listitem>
        <listitem>
          <para><command>numa_foreign</command> is rarely useful but is accounted against a node
            that was preferred. It is a subtle distinction from numa_miss that is rarely
            useful.</para>
        </listitem>
        <listitem>
          <para><command>numa_interleave</command> is incremented when an interleave policy was used
            to select allowed nodes in a round-robin fashion.</para>
        </listitem>
        <listitem>
          <para><command>numa_local</command> increments when a local node is used for an allocation
            regardless of policy.</para>
        </listitem>
        <listitem>
          <para><command>numa_other</command> is used when a remote node is used for an allocation
            regardless of policy.</para>
        </listitem>
      </itemizedlist>

      <para>For the local memory policy, the <command>numa_hit</command> and
          <command>numa_miss</command> counters are the most important to pay attention to. An
        application that is allocating memory that starts incrementing the
          <command>numa_miss</command> implies that the first level of saturation has been reached.
        If this is observed on EPYC 2, it may be valuable to bind the application to nodes that
        represent dies on a single socket. If the ratio of hits to misses is close to 1, consider an
        evaluation of the interleave policy to avoid unnecessary reclaim.</para>

      <note>
        <title>NUMA Statistics</title>
        <para>It is critical to note that these NUMA statistics only apply at the time a physical
          page was allocated and is not related to the reference behaviour of the workload. For
          example, if a task running on node 0 allocates memory local to node 0 then it will be
          accounted for as a <command>node_hit</command> in the statistics. However, if the memory
          is shared with a task running on node 1, all the accesses may be remote, which is a miss
          from the perspective of the hardware but not accounted for in
            <filename>/proc/vmstat</filename>. Detecting remote and local accesses at a hardware
          level requires using the hardwares <emphasis role="italic">Performance Management
            Unit</emphasis> to detect.</para>
      </note>

      <para>When the first saturation point is reached then reclaim will be active. This can be
        observed by monitoring the <command>pgscan_kswapd</command> and
          <command>pgsteal_kswapd</command>
        <filename>/proc/vmstat counters</filename>. If this is matched with an increase in major
        faults or minor faults then it may be indicative of severe thrashing. In this case the
        interleave policy should be considered. An ideal tuning option is to identify if shared
        memory is the source of the usage. If this is the case, then interleave the shared memory
        segments. This can be done in some circumstances using <command>numactl</command> or by
        modifying the application directly.</para>

      <para>More severe saturation is observed if the <command>pgscan_direct</command> and
          <command>pgsteal_direct</command> counters are also increasing as these indicate that the
        application is stalling while memory is being reclaimed. If the application was bound to
        individual nodes, increasing the number of available nodes will alleviate the pressure. If
        the application is unbound, it indicates that the WSS of the workload exceeds all available
        memory. It can only be alleviated by tuning the application to use less memory or increasing
        the amount of RAM available.</para>

      <para>As before, whether to use memory nodes from one socket or two sockets depends on the
        application. If the individual processes are independent then either socket can be used. But
        where possible, keep communicating processes on the same socket to maximize memory
        throughput while minimizing the socket interconnect traffic.</para>

    </sect2>

    <sect2 xml:id="sec.other_resources">
      <title>Other Resources</title>

      <para>The analysis of other resources is outside the scope of this paper. However, a common
        scenario is that an application is IO-bound. A superficial check can be made using the
          <command>vmstat</command> tool and checking what percentage of CPU time is spent idle
        combined with the number of processes that are blocked and the values in the <emphasis
          role="strong">bi</emphasis> and <emphasis role="strong">bo</emphasis> columns. Further
        analysis is required to determine if an application is IO rather than CPU or memory bound.
        But this is a sufficient check to start with. </para>

    </sect2>

  </sect1>
  <sect1 xml:id="sec.power_management">
    <title>Power Management</title>

    <para>Modern CPUs balance power consumption and performance through <emphasis role="italic"
        >Performance States (P-States)</emphasis>. Low utilization workloads may use lower P-States
      to conserve power while still achieving acceptable performance. When a CPU is idle, lower
      power idle states <emphasis role="italic">(C-States)</emphasis> can be selected to further
      conserve power. However this comes with higher exit latencies when lower power states are
      selected. It is further complicated by the fact that if individual cores are idle and running
      at low power then the additional power can be used to boost the performance of active cores.
      This means this scenario is not a straight-forward balance between power consumption and
      performance. More complexity is added on EPYC 2 whereby spare power may be used to boost either
      cores or the Infinity links.</para>

    <para>EPYC 2 provides <emphasis role="italic">SenseMI</emphasis> which, among other capabilities,
      enables CPUs to make adjustments to voltage and frequency depending on the historical state of
      the CPU. There is a latency penalty when switching P-States but EPYC 2 is capable of
      fine-grained in the adjustments that can be made to reduce likelihood that the latency is a
      bottleneck. On SUSE Linux Enterprise Server, EPYC 2 uses the <command>acpi_cpufreq</command>
      driver which allows P-states to be configured to match requested performance. However, this is
      limited in terms of the full capabilities of the hardware. It cannot boost the frequency
      beyond the maximum stated frequencies and if a target is specified then the highest frequency
      below the target will be used. A special case is if the governor is set to <emphasis role="strong">performance</emphasis>. In
      this situation the hardware will use the highest available frequency in an attempt to
      work quickly and then return to idle.</para>

    <para>What should be determined is whether power management is likely to be a factor for a
      workload. One that is limited to a subset of active CPUs and nodes will have high enough
      utilization so that power management will not be active on those cores and no action is
      required. Hence, with CPU binding, the issue of power management may be side-stepped.</para>

    <para>Secondly, a workload that does not communicate heavily with other processes and is mostly
      CPU-bound will also not experience any side effects because of power management.</para>

    <para>The workloads that are most likely to be affected are those that synchronously communicate
      between multiple threads or those that idle frequently and have low CPU utilization overall.
      It will be further compounded if the threads are sensitive to wakeup latency but there are
      secondary effects if a workload must complete quickly but the CPU is running at a low
      frequency.</para>

    <para>The P-State and C-State of each CPU can be examined using the <command>turbostat</command>
      utility. The computer output below shows an example where a workload is busy on CPU 0 and
      other workloads are idle. A useful exercise is to start a workload and monitor the output of
        <command>turbostat</command> paying close attention to CPUs that have moderate utilization
      and running at a lower frequency. If the workload is latency-sensitive then it is grounds for
      either minimizing the number of CPUs available to the workload or configuring power
      management. </para>

   <screen>
Package Core    CPU     Avg_MHz Busy%   Bzy_MHz TSC_MHz IRQ     POLL    C1      C2      POLL%   C1%     C2%
-       -       -       26      0.85    3062    1997    9173    8       81      3900    0.00    0.00    99.19
0       0       0       3190    100.00  3190    1994    1762    0       0       0       0.00    0.00    0.00
0       0       128     1       0.02    3191    1994    22      0       1       14      0.00    0.00    99.88
0       1       1       1       0.00    3195    1997    1773    0       0       0       0.00    0.00    99.94
0       1       129     1       0.01    3195    1997    17      0       1       11      0.00    0.00    99.95
0       2       2       1       0.04    1420    1997    1762    0       0       1272    0.00    0.00    89.76
0       2       130     1       0.02    1420    1997    18      0       1       11      0.00    0.00    100.04
0       3       3       1       0.01    1420    1997    13      0       0       11      0.00    0.00    100.04
0       3       131     1       0.01    1420    1997    17      0       0       11      0.00    0.00    100.04

</screen>

    <para>In the event it is determined that tuning CPU frequency management is appropriate. Then
      the following actions can be taken to set the management policy to performance using the
        <command>cpupower</command> utility:</para>

    <screen>epyc:~# cpupower frequency-set -g performance
Setting cpu: 0 
Setting cpu: 1 
Setting cpu: 2 
...</screen>

    <para>Persisting it across reboots can be done via a local <command>init</command> script, via
        <command>udev</command> or via one-shot <command>systemd</command> service file if it is
      deemed to be necessary. Note that <command>turbostat</command> will still show that idling
      CPUs use a low frequency. The impact of the policy is that the highest P-State will be used as
      soon as possible when the CPU is active. In some cases, a latency bottleneck will occur
      because of a CPU exiting idle. If this is identified on EPYC 2, restrict the C-state by
      specifying <command>processor.max_cstate=2</command> on the kernel command line which will
      prevent CPUs from entering lower C-states. It is expected on EPYC 2 that the exit latency from
      C1 is very low. But by allowing C2, it reduces interference from the idle loop injecting
      micro-operations into the pipeline and should be the best state overall. It is also possible
      to set the max idle state on individual cores using <command>cpupower idle-set</command>. If
      SMT is enabled, the idle state should be set on both siblings.</para>
  </sect1>

  <sect1 xml:id="sec.security_mitigations">
    <title>Security Mitigations</title>

    <para>On occasion, a security fix is applied to a distribution that has a performance impact.
      The most recent notable examples are <emphasis role="strong">Meltdown</emphasis> and two
      variants of <emphasis role="strong">Spectre</emphasis>. AMD EPYC 2 is immune to the Meltdown
      variant and page table isolation is never active. However, it is vulnerable to both Spectre
      variants. The following table lists all security vulnerabilities that affect EPYC 2 and which
      mitgations are enabled by default for SUSE Linux Enterprise Server 15 SP1.</para>

    <table>
      <title>Security mitigations for EPYC 2</title>
      <tgroup cols="3">
        <colspec colname="col_1" colwidth="33*"/>
        <colspec colname="col_2" colwidth="33*"/>
        <colspec colname="col_2" colwidth="33*"/>
        <thead>
          <row>
            <entry>Vulnerability</entry>
            <entry>Affected </entry>
            <entry>Mitigations</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>L1TF</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>MDS</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Meltdown</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Speculative Store Bypass</para>
            </entry>
	    <entry>
              <para>Yes</para>
            </entry>
            <entry>
              <para>prctl and seccomp</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Spectre v1</para>
            </entry>
	    <entry>
              <para>Yes</para>
            </entry>
            <entry>
              <para>usercopy/swapgs barriers and __user pointer sanitization</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Spectre v2</para>
            </entry>
	    <entry>
              <para>Yes</para>
            </entry>
            <entry>
              <para>retpoline, RSB filling, and conditional IBPB, IBRS_FW, and STIBP</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>In the event it can be guaranteed that the server is in a trusted environment running
      only known code that is not malicious, the <parameter>mitigations=off</parameter> parameter can
      be specified on the kernel command line. This option disables all security mitigations and might
      improve performance, but the gains for EPYC 2 will only be marginal when compared against
      other CPUs.</para>
  </sect1>

  <sect1 xml:id="sec.hw_based_profiling">
    <title>Hardware-based Profiling</title>
    <para>EPYC 2 has extensive Performance Monitoring Unit (PMU) capabilities, and
      advanced monitoring of a workload can be conducted via <command>oprofile</command>
      or <command>perf</command>. Both commands support a range of hardware events including
      cycles, L1 cache access/misses, TLB access/misses, retired branch instructions and mispredicted branches.
      In terms of identifying what subsystem may be worth tuning in the OS, the most useful invocation is
      <command>perf record -a -e cycles sleep 30</command> to capture 30 seconds of data for the entire system.
      You can also call <command>perf record -e cycles command</command> to gather a profile of a given
      workload. Specific information on the OS can be gathered through tracepoints or creating probe points with
      <command>perf</command> or <command>trace-cmd</command>. But the details on how to conduct such analysis
      are beyond the scope of this paper.</para> </sect1>

  <sect1 xml:id="sec.candidate_workloads">
    <title>Candidate Workloads</title>
    <para>The workloads that will benefit most from the EPYC 2 architecture are those that can be
      parallelized and are either memory or IO-bound. This is particularly true for workloads that
      are <quote>NUMA friendly</quote>: they can be trivially parallelized and each thread can
      operate independently for the majority of the workloads lifetime. For memory-bound workloads,
      the primary benefit will be taking advantage of the high bandwidth available on each channel.
      For IO-bound workloads, the primary benefit will be realiszed when there are multiple storage
      devices, each of which is connected to the node local to a task issuing IO.</para>

    <sect2 xml:id="sec.test_setup">
      <title>Test Setup</title>

      <para>The following sections will demonstrate how an OpenMP and MPI workload can be configured
        and tuned on an EPYC 2 reference platform.</para>

      <table>
        <title>Test Setup</title>
        <tgroup cols="2">
          <colspec colname="col_1" colwidth="50*"/>
          <colspec colname="col_2" colwidth="50*"/>
          <!--<thead>
            <row>
              <entry/>
              <entry>Balanced</entry>
              <entry>Higher Throughput</entry>
              <entry>Lower Latency</entry>
            </row>
          </thead>-->
          <tbody>
            <row>
              <entry>
                <para>CPU </para>
              </entry>
              <entry>
                <para>2x AMD EPYC 7002</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Platform</para>
              </entry>
              <entry>
                <para>AMD Engineering Sample Platform</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Drive</para>
              </entry>
              <entry>
                <para>Micron 1100 SSD</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OS</para>
              </entry>
              <entry>
                <para>SUSE Linux Enterprise Server 15 SP1</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Memory Interleaving</para>
              </entry>
              <entry>
                <para>Channel</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Memory Speed</para>
              </entry>
              <entry>
                <para>2667MHz (single rank)</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Kernel command line</para>
              </entry>
              <entry>
                <para>
                  <command>mitigations=off</command>
                </para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>

    </sect2>

    <sect2 xml:id="sec.workload_stream">
      <title>Test workload: STREAM</title>

      <para><emphasis role="italic">STREAM</emphasis> is a memory bandwidth benchmark created by Dr.
        John D. McCalpin from the University of Virginia (for more information, see <link
          xlink:href="https://www.cs.virginia.edu/stream/"
          >https://www.cs.virginia.edu/stream/</link>. It can be used to measure bandwidth of each
        cache level and bandwidth to main memory assuming adequate care is taken. It is not perfect
        as some portions of the data will be stored in cache instead of being fetched from main
        memory.</para>

      <para>The benchmark was configured to run both single-threaded and parallelised with OpenMP to
        take advantage of each memory channel. The array elements for the benchmark was set at
        100007936 elements at compile time so each that array was 763MB in size for a total memory
        footprint of 2289 MB. The size was selected to minimize the possibility that cache usage
        would dominate the measurements.</para>

      <table>
        <title>Test Workload: STREAM</title>
        <tgroup cols="2">
          <colspec colname="col_1" colwidth="50*"/>
          <colspec colname="col_2" colwidth="50*"/>
          <!--<thead>
            <row>
            <entry/>
            <entry>Balanced</entry>
            <entry>Higher Throughput</entry>
            <entry>Lower Latency</entry>
            </row>
            </thead>-->
          <tbody>
            <row>
              <entry>
                <para>Compiler</para>
              </entry>
              <entry>
                <para>gcc (SUSE Linux Enterprise) 7.4.1 </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Compiler flags</para>
              </entry>
              <entry>
                <para>
                  <parameter>-m64 -lm -O3</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OpenMP compiler flag</para>
              </entry>
              <entry>
                <para>
                  <parameter>-fopemp</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OpenMP environment variables</para>
              </entry>
              <entry>
                <para>
                  <parameter>OMP_PROC_BIND=SPREAD</parameter>
                </para>
                <para>
                  <parameter>OMP_NUM_THREADS=16</parameter>
                </para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The number of openMP threads was selected to have at least one thread running for every
        memory channel. The <parameter>OMP_PROC_BIND</parameter> parameter was to have one thread
        running on a core with a dedicated L3 cache to maximize available bandwidth. This can be
        verified using <command>trace-cmd</command>, as illustrated below with slight editing for
        formatting and clarity.</para>

      <screen>epyc:~ # trace-cmd record -e sched:sched_migrate_task ./stream
epyc:~ # trace-cmd report
...
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18799 prio=120 orig_cpu=0 dest_cpu=4 
stream-18798 [000]  x: sched_migrate_task:   comm=trace-cmd pid=18670 prio=120 orig_cpu=4 dest_cpu=5
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18800 prio=120 orig_cpu=0 dest_cpu=8 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18801 prio=120 orig_cpu=0 dest_cpu=12 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18802 prio=120 orig_cpu=0 dest_cpu=16 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18803 prio=120 orig_cpu=0 dest_cpu=20 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18804 prio=120 orig_cpu=0 dest_cpu=24 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18805 prio=120 orig_cpu=0 dest_cpu=28 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18806 prio=120 orig_cpu=0 dest_cpu=32 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18807 prio=120 orig_cpu=0 dest_cpu=36 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18808 prio=120 orig_cpu=0 dest_cpu=40 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18809 prio=120 orig_cpu=0 dest_cpu=44 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18810 prio=120 orig_cpu=0 dest_cpu=48 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18811 prio=120 orig_cpu=0 dest_cpu=52 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18812 prio=120 orig_cpu=0 dest_cpu=56 
stream-18798 [000]  x: sched_migrate_task:   comm=stream pid=18813 prio=120 orig_cpu=0 dest_cpu=60  
</screen>

      <para>Figure 2 below shows the reported bandwidth for the single and parallelized case. The
        single-threaded bandwidth for a single core was roughly 28.9 GB/sec which is a high percentage
        of the theoretical max of 42.6 GB/sec (each core has access to two memory channels). The
        channels are interleaved in this configuration as it has been recommended as the best
        balance for a variety of workloads but limits the absolute maximum of a specialized
        benchmark like STREAM. The total throughput for each parallel operation ranged from 180
        GB/sec to 253 GB/sec which is comparable to the theoretical maximum of 341 GB/sec.</para>

      <note>
        <title>STREAM Scores</title>
        <para>Higher STREAM scores can be reported by reducing the array sizes so that cache is
          partially used with the maximum score requiring that each threads memory footprint fits
          inside the L1 cache. Additionally, it is possible to achieve results closer to the
          theoretical maximum by manual optimization of the STREAM benchmark using vectored
          instructions and explicit scheduling of loads and stores. The purpose of this
          configuration was to illustrate the impact of properly binding a workload that can be
          fully parallelized with data-independent threads.</para>
      </note>

      <figure>
        <title>STREAM Bandwidth, Single Threaded and Parallelized</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="amd-epyc-2-graph-stream.png" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="amd-epyc-2-graph-stream.png" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

    </sect2>
  </sect1>

  <sect1 xml:id="sec.conclusion">
    <title>Conclusion</title>

    <para>The introduction of EPYC 2 pushes the boundaries of what is possible for memory and IO-bound
      workloads with much higher bandwidth and available number of channels. A properly configured
      and tuned workload can exceed the performance of many contemporary off-the-shelf solutions
      even when fully customized. The symmetric and balanced nature of the machine makes the task of
      tuning a workload considerably easier given that each partition can have symmetric
      performance.</para>

    <para>With SUSE Linux Enterprise, all the tools to monitor and tune a workload are readily
      available. Your customers can extract the maximum performance and reliability running their
      applications on the EPYC 2 platform.</para>

  </sect1>

  <sect1 xml:id="sec.resources">
    <title>Resources</title>

    <para>For more information, refer to:</para>

    <itemizedlist>
      <listitem>
        <para>AMD SenseMI Technology (<link
            xlink:href="https://www.amd.com/en/technologies/sense-mi"
            >https://www.amd.com/en/technologies/sense-mi</link>)</para>
      </listitem>
      <listitem>
        <para>Balanced power plan optimized for AMD Ryzen processors (<link
            xlink:href="https://community.amd.com/community/gaming/blog/2017/04/06/amd-ryzen-community-update-3"
            >https://community.amd.com/community/gaming/blog/2017/04/06/amd-ryzen-community-update-3</link></para>
      </listitem>
      <listitem>
        <para>EPYC Tech Day: Gerry Talbot (<link
            xlink:href="https://www.youtube.com/watch?v=W5IhEit6NqY"
            >https://www.youtube.com/watch?v=W5IhEit6NqY</link>)</para>
      </listitem>
      <listitem>
        <para>Optimizing Linux for Dual-Core AMD Opteron Processors (<link
            xlink:href="http://www.novell.com/traininglocator/partners/amd/4622016.pdf"
            >http://www.novell.com/traininglocator/partners/amd/4622016.pdf</link>)</para>
      </listitem>
      <listitem>
        <para>Systems Performance: Enterprise and the Cloud by Brendan Gregg (<link
            xlink:href="http://www.brendangregg.com/sysperfbook.html"
            >http://www.brendangregg.com/sysperfbook.html</link>)</para>
      </listitem>
      <listitem>
        <para>NASA Parallel Benchmark (<link
            xlink:href="https://www.nas.nasa.gov/publications/npb.html"
            >https://www.nas.nasa.gov/publications/npb.html</link>)</para>
      </listitem>
    </itemizedlist>

    <para/>

  </sect1>


  <sect1 xml:id="sec.glossary">
    <title>Glossary</title>

    <para/>

  </sect1>





  <sect1 xml:id="sec.appendix_a">
    <title>Appendix A</title>

    <para>Example of a VM configuration file:</para>

    <screen>
  &lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>  
  &lt;name>sles12sp3_01&lt;/name>
    &lt;uuid>26137bb8-9e5f-48e9-a81d-63ae36400196&lt;/uuid>
    &lt;memory unit='KiB'>209715200&lt;/memory>
    &lt;currentMemory unit='KiB'>209715200&lt;/currentMemory>
    &lt;memoryBacking>
      &lt;hugepages>
        &lt;page size='1048576' unit='KiB'/>
      &lt;/hugepages>
      &lt;nosharepages/>
    &lt;/memoryBacking>
    &lt;vcpu placement='static'>96&lt;/vcpu>
    &lt;cputune>
      &lt;vcpupin vcpu='0' cpuset='1'/>
      &lt;vcpupin vcpu='1' cpuset='65'/>
      &lt;vcpupin vcpu='2' cpuset='2'/>
      &lt;vcpupin vcpu='3' cpuset='66'/>
      &lt;vcpupin vcpu='4' cpuset='3'/>
      &lt;vcpupin vcpu='5' cpuset='67'/>
      &lt;vcpupin vcpu='6' cpuset='4'/>
      &lt;vcpupin vcpu='7' cpuset='68'/>
      &lt;vcpupin vcpu='8' cpuset='5'/>
      &lt;vcpupin vcpu='9' cpuset='69'/>
      &lt;vcpupin vcpu='10' cpuset='6'/>
      &lt;vcpupin vcpu='11' cpuset='70'/>
      &lt;vcpupin vcpu='12' cpuset='9'/>
      &lt;vcpupin vcpu='13' cpuset='73'/>
      &lt;vcpupin vcpu='14' cpuset='10'/>
      &lt;vcpupin vcpu='15' cpuset='74'/>
      &lt;vcpupin vcpu='16' cpuset='11'/>
      &lt;vcpupin vcpu='17' cpuset='75'/>
      &lt;vcpupin vcpu='18' cpuset='12'/>
      &lt;vcpupin vcpu='19' cpuset='76'/>
      &lt;vcpupin vcpu='20' cpuset='13'/>
      &lt;vcpupin vcpu='21' cpuset='77'/>
      &lt;vcpupin vcpu='22' cpuset='14'/>
      &lt;vcpupin vcpu='23' cpuset='78'/>
      &lt;vcpupin vcpu='24' cpuset='17'/>
      &lt;vcpupin vcpu='25' cpuset='81'/>
      &lt;vcpupin vcpu='26' cpuset='18'/>
      &lt;vcpupin vcpu='27' cpuset='82'/>
      &lt;vcpupin vcpu='28' cpuset='19'/>
      &lt;vcpupin vcpu='29' cpuset='83'/>
      &lt;vcpupin vcpu='30' cpuset='20'/>
      &lt;vcpupin vcpu='31' cpuset='84'/>
      &lt;vcpupin vcpu='32' cpuset='21'/>
      &lt;vcpupin vcpu='33' cpuset='85'/>
      &lt;vcpupin vcpu='34' cpuset='22'/>
      &lt;vcpupin vcpu='35' cpuset='86'/>
      &lt;vcpupin vcpu='36' cpuset='25'/>
      &lt;vcpupin vcpu='37' cpuset='89'/>
      &lt;vcpupin vcpu='38' cpuset='26'/>
      &lt;vcpupin vcpu='39' cpuset='90'/>
      &lt;vcpupin vcpu='40' cpuset='27'/>
      &lt;vcpupin vcpu='41' cpuset='91'/>
      &lt;vcpupin vcpu='42' cpuset='28'/>
      &lt;vcpupin vcpu='43' cpuset='92'/>
      &lt;vcpupin vcpu='44' cpuset='29'/>
      &lt;vcpupin vcpu='45' cpuset='93'/>
      &lt;vcpupin vcpu='46' cpuset='30'/>
      &lt;vcpupin vcpu='47' cpuset='94'/>
      &lt;vcpupin vcpu='48' cpuset='33'/>
      &lt;vcpupin vcpu='49' cpuset='97'/>
      &lt;vcpupin vcpu='50' cpuset='34'/>
      &lt;vcpupin vcpu='51' cpuset='98'/>
      &lt;vcpupin vcpu='52' cpuset='35'/>
      &lt;vcpupin vcpu='53' cpuset='99'/>
      &lt;vcpupin vcpu='54' cpuset='36'/>
      &lt;vcpupin vcpu='55' cpuset='100'/>
      &lt;vcpupin vcpu='56' cpuset='37'/>
      &lt;vcpupin vcpu='57' cpuset='101'/>
      &lt;vcpupin vcpu='58' cpuset='38'/>
      &lt;vcpupin vcpu='59' cpuset='102'/>
      &lt;vcpupin vcpu='60' cpuset='41'/>
      &lt;vcpupin vcpu='61' cpuset='105'/>
      &lt;vcpupin vcpu='62' cpuset='42'/>
      &lt;vcpupin vcpu='63' cpuset='106'/>
      &lt;vcpupin vcpu='64' cpuset='43'/>
      &lt;vcpupin vcpu='65' cpuset='107'/>
      &lt;vcpupin vcpu='66' cpuset='44'/>
      &lt;vcpupin vcpu='67' cpuset='108'/>
      &lt;vcpupin vcpu='68' cpuset='45'/>
      &lt;vcpupin vcpu='69' cpuset='109'/>
      &lt;vcpupin vcpu='70' cpuset='46'/>
      &lt;vcpupin vcpu='71' cpuset='110'/>
      &lt;vcpupin vcpu='72' cpuset='49'/>
      &lt;vcpupin vcpu='73' cpuset='113'/>
      &lt;vcpupin vcpu='74' cpuset='50'/>
      &lt;vcpupin vcpu='75' cpuset='114'/>
      &lt;vcpupin vcpu='76' cpuset='51'/>
      &lt;vcpupin vcpu='77' cpuset='115'/>
      &lt;vcpupin vcpu='78' cpuset='52'/>
      &lt;vcpupin vcpu='79' cpuset='116'/>
      &lt;vcpupin vcpu='80' cpuset='53'/>
      &lt;vcpupin vcpu='81' cpuset='117'/>
      &lt;vcpupin vcpu='82' cpuset='54'/>
      &lt;vcpupin vcpu='83' cpuset='118'/>
      &lt;vcpupin vcpu='84' cpuset='57'/>
      &lt;vcpupin vcpu='85' cpuset='121'/>
      &lt;vcpupin vcpu='86' cpuset='58'/>
      &lt;vcpupin vcpu='87' cpuset='122'/>
      &lt;vcpupin vcpu='88' cpuset='59'/>
      &lt;vcpupin vcpu='89' cpuset='123'/>
      &lt;vcpupin vcpu='90' cpuset='60'/>
      &lt;vcpupin vcpu='91' cpuset='124'/>
      &lt;vcpupin vcpu='92' cpuset='61'/>
      &lt;vcpupin vcpu='93' cpuset='125'/>
      &lt;vcpupin vcpu='94' cpuset='62'/>
      &lt;vcpupin vcpu='95' cpuset='126'/>
    &lt;/cputune>
    &lt;numatune>
      &lt;memory mode='strict' nodeset='0-7'/>
      &lt;memnode cellid='0' mode='strict' nodeset='0'/>
      &lt;memnode cellid='1' mode='strict' nodeset='1'/>
      &lt;memnode cellid='2' mode='strict' nodeset='2'/>
      &lt;memnode cellid='3' mode='strict' nodeset='3'/>
      &lt;memnode cellid='4' mode='strict' nodeset='4'/>
      &lt;memnode cellid='5' mode='strict' nodeset='5'/>
      &lt;memnode cellid='6' mode='strict' nodeset='6'/>
      &lt;memnode cellid='7' mode='strict' nodeset='7'/>
    &lt;/numatune>
    &lt;os>
      &lt;type arch='x86_64' machine='pc-i440fx-2.9'>hvm&lt;/type>
      &lt;boot dev='hd'/>
    &lt;/os>
    &lt;features>
      &lt;acpi/>
      &lt;apic/>
    &lt;/features>
    &lt;cpu mode='host-passthrough' check='none'>
      &lt;topology sockets='8' cores='6' threads='2'/>
      &lt;numa>
        &lt;cell id='0' cpus='0-11' memory='26214400' unit='KiB'/>
        &lt;cell id='1' cpus='12-23' memory='26214400' unit='KiB'/>
        &lt;cell id='2' cpus='24-35' memory='26214400' unit='KiB'/>
        &lt;cell id='3' cpus='36-47' memory='26214400' unit='KiB'/>
        &lt;cell id='4' cpus='48-59' memory='26214400' unit='KiB'/>
        &lt;cell id='5' cpus='60-71' memory='26214400' unit='KiB'/>
        &lt;cell id='6' cpus='72-83' memory='26214400' unit='KiB'/>
        &lt;cell id='7' cpus='84-95' memory='26214400' unit='KiB'/>
      &lt;/numa>
    &lt;/cpu>
    ...
    &lt;devices>
      &lt;emulator>/usr/bin/qemu-kvm&lt;/emulator>
      &lt;disk type='file' device='disk'>
        &lt;driver name='qemu' type='qcow2'/>
        &lt;source file='/home/sles12sp3_01.img'/>
        &lt;target dev='vda' bus='virtio'/>
        &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
      &lt;/disk>
      ...
      &lt;interface type='network'>
        &lt;mac address='52:54:00:9e:08:44'/>
        &lt;source network='default'/>
        &lt;model type='virtio'/>
        &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
      &lt;/interface>
      ...
      &lt;rng model='virtio'>
        &lt;backend model='random'>/dev/urandom&lt;/backend>
        &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
      &lt;/rng>
    &lt;/devices>
    &lt;qemu:commandline>
      &lt;qemu:arg value='-cpu'/>
      &lt;qemu:arg value='host,migratable=off,+invtsc,l3-cache=on'/>
    &lt;/qemu:commandline>
  &lt;/domain>
</screen>

  </sect1>

  <sect1 xml:id="sec.legal_notice">
    <title>Legal Notice</title>
    <para>Copyright &copy;2006–2019 SUSE LLC and contributors. All rights reserved. </para>
    <para>Permission is granted to copy, distribute and/or modify this document under the terms of
      the GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the
      Invariant Section being this copyright notice and license. A copy of the license version 1.2
      is included in the section entitled <quote>GNU Free Documentation License</quote>.</para>
    <para>SUSE, the SUSE logo and YaST are registered trademarks of SUSE LLC in the United States
      and other countries. For SUSE trademarks, see <link
        xlink:href="http://www.suse.com/company/legal/">http://www.suse.com/company/legal/</link>.
      Linux is a registered trademark of Linus Torvalds. All other names or trademarks mentioned in
      this document may be trademarks or registered trademarks of their respective owners.</para>
    <para>This article is part of a series of documents called "SUSE Best Practices". The individual
      documents in the series were contributed voluntarily by SUSE's employees and by third
      parties.</para>
    <!--  <para>The articles are intended only to be one example of how a particular action could be
      taken. They should not be understood to be the only action and certainly not to be the
      action recommended by SUSE. Also, SUSE cannot verify either that the actions described
      in the articles do what they claim to do or that they don't have unintended
      consequences.</para>-->
    <para> All information found in this article has been compiled with utmost attention to detail.
      However, this does not guarantee complete accuracy.
      <!--Neither SUSE LLC, the authors, nor the translators shall be held liable
        for possible errors or the consequences thereof. --></para>
    <para>Therefore, we need to specifically state that neither SUSE LLC, its affiliates, the
      authors, nor the translators may be held liable for possible errors or the consequences
      thereof. Below we draw your attention to the license under which the articles are
      published.</para>
  </sect1>
  <?pdfpagebreak style="suse2013" formatter="fop"?>
  <xi:include href="license-gfdl.xml"/>
</article>
